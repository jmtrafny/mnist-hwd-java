# MNIST Dataset Classifier
## OVERVIEW
This project is a handwritten digit classifier for the MNIST Dataset.  The dataset is a collection of 60,000 images (and labels) of handwritten digits.  The images have been preprocessed to align and center the images.  The images are represented in high endian format and can be downloaded from http://yann.lecun.com/exdb/mnist/.  To classify the images, the raw data from each image in the data set is used to train an artificial neural network (ANN).  The ANN is a 2-layer network, meaning 1-hidden and 1-output layer, the input layer is not counted.  The input layer consists of 784 “neurons”, one for each pixel of the image.  Each pixel is represented with a value between 0 and 255 representing the brightness of the pixel.  There are 28 neurons in the hidden layer, this number was arbitrarily chosen because it achieves success rates close to the 300-node setup recommended by MNIST, but trains in a fraction of the time.  The output layer consists of 10 nodes, the node with the highest value represents the output.
The software designed for this project consists of three java classes; MnistReader.java, NeuralNetwork.java, and Main.java.  The MnistReader file is an open source tool created by Jeff Griffith and is used to extract the data from the MNIST image and label files and return Lists of 2D int arrays representing each digit, and a 1D int array representing each label.  The NeuralNetwork file contains all the code that represents the neural network.  It has a public constructor function, a public train function, and a public test function.  It also has other private functions for internal computation.  The Main file is the entry point for the program and is responsible for instantiating our network and providing the user with feedback.
# OPERATION OF THE SOFTWARE
Operation begins in the main method of the main class.  The required data files must be in the “data” folder in the same directory as “src”.  The “getImages()” and “getLabels()” functions in the MnistReader class are called to prepare our image and label data for the neural network.  A neural network object is then instantiated, using the desired size of each layer as parameters.  The size of the input layer is 784, on input neuron for each pixel in the image.  For the hidden layer, it is hard to determine an optimal size, however the MNIST website shows testing ANNs with 300 hidden neurons achieved accuracy’s of ~80%.  However, using just 28 neurons, I was able to achieve ~77% with much less computation time.  The output layer has 10 neurons, each neuron represents one digit, 0-9, and after each input, the neuron with the highest value is the guess for that input.
The NeuralNetwork file has two main functions; train() and test().  The train() function is uses the 60,000 images in the training set to adjust its weights using backpropagation.  The test() function uses the 10,000 images in the testing set to test how well its weights work for the problem.
Training begins by identifying what the label should be and encoding it in an int array of size ten.  The image values, stored in a 2D int array, are flattened out into an array called “input_vals”.  This array represents our input layer.  The next step is to calculate the output values of the hidden layer.  This is done by summing the value of each input value, times its hidden weight, minus a threshold value.  This summation is “activated” using the sigmoid function.  This process is repeated to get the values of the output layer, but this time using the output weights and output threshold.  The output layer now holds our encoded answer, which gets converted to an int for tracking the training process.  
The error gradient for each output node is calculated as a function of the guessed answer and the correct answer.  This error is backpropagated through the connected nodes and used to adjust the weights after each input.  The test() function works similar to the train() function, however it does not need to apply any of the error correction mechanisms.  
The system outputs three files, one for the training results, one for the testing results, and one for the meta data containing all the weight values.  These files are saved in the “results” folder in the same directory as the “src” file.  
 
